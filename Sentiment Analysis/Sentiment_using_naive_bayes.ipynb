{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b27aba-f8a0-4c67-9269-55063c4d4ab0",
   "metadata": {
    "id": "04b27aba-f8a0-4c67-9269-55063c4d4ab0"
   },
   "source": [
    "## Dataset and installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9420838-ea81-46ee-ba0f-7a54146ba47e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9420838-ea81-46ee-ba0f-7a54146ba47e",
    "outputId": "226eaf4a-a5ae-4e2f-8842-505ed61511d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3062e91-d7f5-499d-99fa-e4f08c457713",
   "metadata": {
    "id": "e3062e91-d7f5-499d-99fa-e4f08c457713"
   },
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa9ef0f0-44dc-4ce8-95d4-0ea82d2129a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa9ef0f0-44dc-4ce8-95d4-0ea82d2129a4",
    "outputId": "1cedb9ad-edd5-4c5c-d98c-7dbc9182656a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_positive_tweets))\n",
    "print(type(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d24d37-16d7-424f-9966-b64ee5f62372",
   "metadata": {
    "id": "70d24d37-16d7-424f-9966-b64ee5f62372"
   },
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c67934-f91d-46df-81f0-7ccce58e1ba6",
   "metadata": {
    "id": "22c67934-f91d-46df-81f0-7ccce58e1ba6"
   },
   "source": [
    "## Preprocessing of the tweets using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547fc1fa-9dfd-4185-bb52-58695a3a5dad",
   "metadata": {
    "id": "547fc1fa-9dfd-4185-bb52-58695a3a5dad"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)\n",
    "    tweet2 = re.sub(r'#', '', tweet2)\n",
    "    tokenizer = TweetTokenizer(preserve_case = False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "    tweets_clean = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    stemmer = PorterStemmer()\n",
    "    tweets_stem = []\n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "\n",
    "    return tweets_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96fc64b9-f053-41fb-8f83-d5e7fb9a56b5",
   "metadata": {
    "id": "96fc64b9-f053-41fb-8f83-d5e7fb9a56b5"
   },
   "outputs": [],
   "source": [
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa7017d-710f-4355-9e5e-0924d60f5b19",
   "metadata": {
    "id": "caa7017d-710f-4355-9e5e-0924d60f5b19"
   },
   "outputs": [],
   "source": [
    "def build_frequency(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4061333d-39c0-49e6-a2a6-556f14a6f2fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4061333d-39c0-49e6-a2a6-556f14a6f2fd",
    "outputId": "344dd359-6071-45b0-f7fa-f07897f45975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 13170\n"
     ]
    }
   ],
   "source": [
    "freqs = build_frequency(tweets, labels)\n",
    "\n",
    "# check data type\n",
    "print(f'type(freqs) = {type(freqs)}')\n",
    "\n",
    "# check length of the dictionary\n",
    "print(f'len(freqs) = {len(freqs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b82316-466f-41ef-941a-b3bc46676f3c",
   "metadata": {
    "id": "d7b82316-466f-41ef-941a-b3bc46676f3c"
   },
   "outputs": [],
   "source": [
    "def get_ratio(freqs, word):\n",
    "\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0000}\n",
    "    ### START CODE HERE ###\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = freqs.get((word, 1), 0)\n",
    "\n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = freqs.get((word, 0), 0)\n",
    "\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1) / (pos_neg_ratio['negative'] + 1)\n",
    "\n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440aae12-5fc8-451f-a61e-3f7080d799ca",
   "metadata": {
    "id": "440aae12-5fc8-451f-a61e-3f7080d799ca"
   },
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word , y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bdf01eb-ed5e-4d1c-857f-79d7ed0c3f05",
   "metadata": {
    "id": "4bdf01eb-ed5e-4d1c-857f-79d7ed0c3f05"
   },
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "\n",
    "    logprior = 0\n",
    "    loglikelihood = {}\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = sum(train_y)\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos / D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = freqs.get((word, 1),0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log( p_w_pos / p_w_neg )\n",
    "\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e40d5cb3-06dd-4b84-80e2-9b4dcede8835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e40d5cb3-06dd-4b84-80e2-9b4dcede8835",
    "outputId": "848b8d17-01ea-4ce6-bff5-9cfc5f857e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "10504\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30d8d996-5f34-46a5-a526-c102b415c414",
   "metadata": {
    "id": "30d8d996-5f34-46a5-a526-c102b415c414"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    word_l = process_tweet(tweet)\n",
    "    p = 0\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe4571d-cce1-4103-9159-b8645704bae2",
   "metadata": {
    "id": "5fe4571d-cce1-4103-9159-b8645704bae2"
   },
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "\n",
    "    accuracy = 0  # return this properly\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            y_hat_i = 0\n",
    "        y_hats.append(y_hat_i)\n",
    "    error = np.mean(np.abs(np.array(y_hats) - np.array(test_y)))\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f2b8140-4aee-48c5-8583-5c0a4181b8a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f2b8140-4aee-48c5-8583-5c0a4181b8a4",
    "outputId": "c59da7ce-ded8-4210-c8fd-345a9503393f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1381072517230804\n"
     ]
    }
   ],
   "source": [
    "my_tweet = \"Just finished my internship building a real-time voice-to-voice translator! ️bad experience for a future where language barriers are broken through the power of AI!  #NLP #AI #futureoftech\"\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
